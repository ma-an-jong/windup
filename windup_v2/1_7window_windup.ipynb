{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e19e54a-c5e0-4347-b517-f38653c5528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "from torch.optim import AdamW,Adam\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a846df8-dd84-4d0b-9d8a-ceec46f925fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel,AutoTokenizer,BertModel\n",
    "from kobert_tokenizer import KoBERTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9303b0-6582-481c-a91c-c66e19bf26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'skt/kobert-base-v1' #multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d5c78b-6fdd-4c06-a1a8-7d0a5c435728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_path)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(768, 1024)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(1024,1)\n",
    "        \n",
    "    def forward(self, input_id, mask):\n",
    "        last_hidden_state,_ = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout1(last_hidden_state)\n",
    "        linear_output = self.linear1(dropout_output)\n",
    "        dropout_output1 = self.dropout2(linear_output)\n",
    "        linear_output1 = self.linear2(dropout_output1)\n",
    "        sigmoid_output = torch.sigmoid(linear_output1)\n",
    "        return sigmoid_output[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf48687-4f1e-4fc1-9de1-8667f0be32ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "classifier = BertClassifier()\n",
    "classifier.load_state_dict(torch.load('/home/se/paper/classifier/kobert_base_model/model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a30f331-caec-45cb-9bcc-fc4ff0520995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(context,null_char='',space_char=' '):\n",
    "    bracket_removed_context = re.sub(r'\\([^)]*\\)', null_char, context)\n",
    "    square_bracket_removed_context = re.sub(r'\\[[^]]*\\]', null_char, bracket_removed_context)\n",
    "    punc_removed_context = re.sub(r'[^\\w\\s]',space_char,square_bracket_removed_context)\n",
    "    repeated_space_removed_context = re.sub(r'[\\n]{2,}','\\n',punc_removed_context)\n",
    "    repeated_space_removed_context.upper()\n",
    "            \n",
    "    return repeated_space_removed_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c2ae5de-ae53-4af2-8e0e-90b6415db8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(contents):\n",
    "    model_input = tokenizer(preprocess(contents),padding='max_length', max_length = 512, truncation=True,return_tensors=\"pt\")\n",
    "    mask = model_input['attention_mask']\n",
    "    input_ids = model_input['input_ids']\n",
    "    label = classifier(input_ids,mask)\n",
    "    return round(label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7e22f5-ef44-40fa-8107-c3b6a794f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/se/paper/legacy/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dba68a2-81ce-45ba-be9e-05fca071a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatDataset(Dataset):\n",
    "    def __init__(self, match,stat,article,le):\n",
    "        \n",
    "        tmp = []\n",
    "        label = []\n",
    "        \n",
    "        for home_pitcher,home_team,away_pitcher,away_team,home_result,date in \\\n",
    "        list(zip(match['home_pitcher'].values,match['home'].values,match['away_pitcher'],match['away'],match['home_result'],match['date'])):\n",
    "            home_player_stat = stat.loc[(stat['name'] == home_pitcher) & (stat['team'] == home_team)]\n",
    "            away_player_stat = stat.loc[(stat['name'] == away_pitcher) & (stat['team'] == away_team)]\n",
    "\n",
    "            if (len(home_player_stat) != 0) and (len(away_player_stat) != 0):\n",
    "                vector = home_player_stat.values.tolist()[0][2:] + away_player_stat.values.tolist()[0][2:]\n",
    "                str_date = datetime.strptime(str(date),\"%Y%m%d\")\n",
    "                home_article = article.loc[(article['team'] == home_team) & (article['date'] > int((str_date - timedelta(days=7)).strftime(\"%Y%m%d\"))) & (article['date'] < date)]\n",
    "                away_article = article.loc[(article['team'] == away_team) & (article['date'] > int((str_date - timedelta(days=7)).strftime(\"%Y%m%d\"))) & (article['date'] < date)]\n",
    "                if (len(home_article) != 0) and (len(away_article) != 0):\n",
    "                    home_sum = 0\n",
    "                    for index,document in home_article.iterrows():\n",
    "                        home_sum += predict(document.contents)\n",
    "\n",
    "                    away_sum = 0\n",
    "                    for index,document in away_article.iterrows():\n",
    "                        away_sum += predict(document.contents)\n",
    "\n",
    "                    vector.append(home_sum / len(home_article))\n",
    "                    vector.append(away_sum / len(away_article))\n",
    "\n",
    "                    tmp.append(vector)\n",
    "                    label.append(home_result)\n",
    "        \n",
    "        transform = le.transform(label)\n",
    "        self.label = torch.tensor(list(transform),dtype=torch.long)\n",
    "        self.stat = torch.tensor(tmp)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.stat)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "                \"stat\": self.stat[index],\n",
    "                \"label\": self.label[index]\n",
    "                }\n",
    "        \n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5a96e28-2f37-4c4d-8ef8-4c2636b215cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindupClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(WindupClassifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(38, 1024)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(1024,3)\n",
    "        \n",
    "    def forward(self, input_id):\n",
    "        linear_output = self.linear1(input_id)\n",
    "        dropout_output1 = self.dropout2(linear_output)\n",
    "        linear_output1 = self.linear2(dropout_output1)\n",
    "        return linear_output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba62c468-c630-4c1e-ac0f-8dcf900594a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = pd.read_csv(os.path.join(data_path,'average.csv'))\n",
    "match = pd.read_csv(os.path.join(data_path,'match_day.csv'))\n",
    "article = pd.read_csv(os.path.join(data_path,'article.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b008c8-d1dc-4322-85bd-542f43b66ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "_=le.fit(['승','패','무승부'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb62a444-e32e-4dde-8195-0cd908effdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_match,valid_match = train_test_split(match,shuffle=True,random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd2fbd-fd16-45f3-8460-52938a39e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StatDataset(train_match,stat,article,le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e67b8-607e-4e2d-b7cf-e85aa48797a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = StatDataset(valid_match,stat,article,le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea6ae2-edac-412a-aab4-8ea1b21d10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset),len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e77be6-02c9-4cc0-ba92-eb69d461ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 100\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773d4d7-1fbe-46e2-b261-ca7c2fb657b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416dbb24-efd3-41c8-acc0-35e65687d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class = pd.DataFrame(train_dataset.label)\n",
    "train_class_counts = [len(train_class.loc[train_class[0] == 0]) , len(train_class.loc[train_class[0] == 1])  , len(train_class.loc[train_class[0] == 2])]\n",
    "train_num_samples = sum(train_class_counts)\n",
    "train_labels = list(train_class.values.reshape(-1))\n",
    "\n",
    "train_class_weights = [train_num_samples / train_class_counts[i] for i in range(len(train_class_counts))] \n",
    "\n",
    "# 해당 데이터의 label에 해당되는 가중치\n",
    "train_weights = [train_class_weights[train_labels[i]] for i in range(int(train_num_samples))] #해당 레이블마다의 가중치 비율\n",
    "train_sampler = WeightedRandomSampler(torch.DoubleTensor(train_weights), int(train_num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ebcb5d-af9d-4402-9d43-ce1ddfaca377",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_class = pd.DataFrame(valid_dataset.label)\n",
    "valid_class_counts = [len(valid_class.loc[train_class[0] == 0]) , len(valid_class.loc[train_class[0] == 1])  , len(valid_class.loc[train_class[0] == 2])]\n",
    "valid_num_samples = sum(valid_class_counts)\n",
    "valid_labels = list(valid_class.values.reshape(-1))\n",
    "\n",
    "valid_class_weights = [valid_num_samples / valid_class_counts[i] for i in range(len(valid_class_counts))] \n",
    "\n",
    "# 해당 데이터의 label에 해당되는 가중치\n",
    "valid_weights = [valid_class_weights[valid_labels[i]] for i in range(int(valid_num_samples))] #해당 레이블마다의 가중치 비율\n",
    "valid_sampler = WeightedRandomSampler(torch.DoubleTensor(valid_weights), int(valid_num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464700c-c9f3-4a1a-a26c-f0b411862ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = WindupClassifier()\n",
    "classifier.cuda()\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927636b1-85f4-41b7-b8e2-f9175b1c0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "_=criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fe9a8-b829-41bf-8a0f-b0c3d16249ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,sampler = train_sampler)\n",
    "val_dataloader = DataLoader(valid_dataset, batch_size=batch_size,shuffle=False,sampler = valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39a303-bd2c-453a-a57e-358bfdd13b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "valid_loss = []\n",
    "valid_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045548c7-cc63-43ba-8020-8b4d97c7de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.zero_grad()\n",
    "for epoch in tqdm(range(epochs),total=epochs):\n",
    "\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    classifier.train()\n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "\n",
    "        train_label = batch['label'].to(device)\n",
    "        train_stat = batch['stat'].to(device)\n",
    "        \n",
    "        output = classifier(train_stat)\n",
    "        output = output.squeeze(1)\n",
    "        batch_loss = criterion(output.float(), train_label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss_train += batch_loss.item()\n",
    "        \n",
    "        correct_prediction = torch.argmax(F.softmax(output, dim=1), 1) == train_label\n",
    "        acc = correct_prediction.float().mean()\n",
    "#         acc = (output.round() == train_label).sum().item()\n",
    "        total_acc_train += acc.item()\n",
    "        \n",
    "    train_loss.append(total_loss_train/len(train_dataloader))\n",
    "    train_acc.append(total_acc_train/len(train_dataloader))\n",
    "            \n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    \n",
    "    with  torch.no_grad():\n",
    "        classifier.eval()\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            \n",
    "            val_label = batch['label'].to(device)\n",
    "            val_stat = batch['stat'].to(device)\n",
    "\n",
    "            output = classifier(val_stat)\n",
    "            output = output.squeeze(1)\n",
    "            batch_loss = criterion(output.float(), val_label)\n",
    "            \n",
    "            total_loss_val += batch_loss.item()\n",
    "            correct_prediction = torch.argmax(F.softmax(output, dim=1), 1) == val_label\n",
    "            acc = correct_prediction.float().mean()\n",
    "#             acc = (output.round() == val_label).sum().item()\n",
    "            \n",
    "            total_acc_val += acc.item()\n",
    "\n",
    "        valid_loss.append(total_loss_val/ len(val_dataloader))\n",
    "        valid_acc.append(total_acc_val/ len(val_dataloader))\n",
    "#         torch.save(classifier.state_dict(), './model{}.pt'.format(epoch))\n",
    " \n",
    "        print(\n",
    "                f'Epochs: {epoch + 1} | Train Loss: {total_loss_train / len(train_dataloader): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_dataloader): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_dataloader): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_dataloader): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9bef0f-d743-45e7-8976-64b6b1b7f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bfcee-97a7-412a-9928-1ac306412dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_loss,color='red',label = 'valid')\n",
    "plt.plot(train_loss,color='blue',label = 'train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dcfafa-d7e0-4976-a6fc-92c60c1a10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_acc,color='red',label = 'valid')\n",
    "plt.plot(train_acc,color='blue',label = 'train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43af3ca-b9a7-46d3-875c-cc5025406290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
